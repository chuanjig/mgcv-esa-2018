GAMs: Model Selection
============================
author: Gavin L Simpson
date: August 5th, 2018
css: custom.css
transition: none

Overview
=========

- Model selection
- Shrinkage via double penalty (`select = TRUE`)
- Shrinkage smooths
- Confidence intervals for smooths
- *p* values
- `anova()`
- AIC

```{r setup, include=FALSE}
library("knitr")
library("viridis")
library("ggplot2")
library("mgcv")
library("cowplot")
theme_set(theme_bw())
opts_chunk$set(cache=TRUE, echo=FALSE)
```

Model selection
===============
type:section

Model selection
===============

Model (or variable) selection --- an important area of theoretical and applied interest

- In statistics we aim for a balance between *fit* and *parsimony*
- In applied research we seek the set of covariates with strongest effects on $y$

We seek a subset of covariates that improves *interpretability* and *prediction accuracy*

Shrinkage & additional penalties
================================
type:section

Shrinkage & additional penalties
================================

Smoothing parameter estimation allows selection of a wide range of potentially complex functions for smooths...

But, cannot remove a term entirely from the model because the penalties used act only on the *range space* of a spline basis. The *null space* of the basis is unpenalised.

- **Null space** --- the basis functions that are smooth (constant, linear)
- **Range space** --- the basis functions that are wiggly

Shrinkage & additional penalties
================================

**mgcv** has two ways to penalize the null space, i.e. to do selection

- *double penalty approach* via `select = TRUE`
- *shrinkage approach* via special bases for
    - thin plate spline (default, `s(..., bs = 'ts')`),
    - cubic splines  (`s(..., bs = 'cs')`)

**double penalty** tends to works best, but applies to all smooths *and* doubles the number of smoothness parameters to estimate

Other shrinkage/selection approaches are available in other software

<!-- Double-penalty shrinkage -->
<!-- ======================== -->

<!-- $\mathbf{S}_j$ is the smoothing penalty matrix & can be decomposed as -->

<!-- $$ -->
<!-- \mathbf{S}_j = \mathbf{U}_j\mathbf{\Lambda}_j\mathbf{U}_j^{T} -->
<!-- $$ -->

<!-- where $\mathbf{U}_j$ is a matrix of eigenvectors and $\mathbf{\Lambda}_j$ a diagonal matrix of eigenvalues (i.e. this is an eigen decomposition of $\mathbf{S}_j$). -->

<!-- $\mathbf{\Lambda}_j$ contains some **0**s due to the spline basis null space --- no matter how large the penalty $\lambda_j$ might get no guarantee a smooth term will be suppressed completely. -->

<!-- To solve this we need an extra penalty... -->

<!-- Double-penalty shrinkage -->
<!-- ======================== -->

<!-- Create a second penalty matrix from $\mathbf{U}_j$, considering only the matrix of eigenvectors associated with the zero eigenvalues -->

<!-- $$ -->
<!-- \mathbf{S}_j^{*} = \mathbf{U}_j^{*}\mathbf{U}_j^{*T} -->
<!-- $$ -->

<!-- Now we can fit a GAM with two penalties of the form -->

<!-- $$ -->
<!-- \lambda_j \mathbf{\beta}^T \mathbf{S}_j \mathbf{\beta} + \lambda_j^{*} \mathbf{\beta}^T \mathbf{S}_j^{*} \mathbf{\beta} -->
<!-- $$ -->

<!-- Which implies two sets of penalties need to be estimated. -->

<!-- In practice, add `select = TRUE` to your `gam()` call -->

<!-- Shrinkage -->
<!-- ========= -->

<!-- The double penalty approach requires twice as many smoothness parameters to be estimated. An alternative is the shrinkage approach, where $\mathbf{S}_j$ is replaced by -->


<!-- $$ -->
<!-- \tilde{\mathbf{S}}_j = \mathbf{U}_j\tilde{\mathbf{\Lambda}}_j\mathbf{U}_j^{T} -->
<!-- $$ -->

<!-- where $\tilde{\mathbf{\Lambda}}_j$ is as before except the zero eigenvalues are set to some small value $\epsilon$. -->

<!-- This allows the null space terms to be shrunk by the standard smoothing parameters. -->

<!-- Use `s(..., bs = "ts")` or `s(..., bs = "cs")` in **mgcv** -->

Empirical Bayes...?
===================

$\mathbf{S}_j$ can be viewed as prior precision matrices and $\lambda_j$ as improper Gaussian priors on the spline coefficients.

The impropriety derives from $\mathbf{S}_j$ not being of full rank (zeroes in $\mathbf{\Lambda}_j$).

Both the double penalty and shrinkage smooths remove the impropriety from the Gaussian prior

Empirical Bayes...?
===================

- **Double penalty** --- makes no assumption as to how much to shrink the null space. This is determined from the data via estimation of $\lambda_j^{*}$
- **Shrinkage smooths** --- assumes null space should be shrunk less than the wiggly part

Marra & Wood (2011) show that the double penalty and the shrinkage smooth approaches

- performed significantly better than alternatives in terms of *predictive ability*, and
- performed as well as alternatives in terms of variable selection

Example
=======
left: 60%
```{r setup-shrinkage-example, echo = FALSE, include = FALSE}
## an example of automatic model selection via null space penalization
set.seed(3)
n <- 200
dat <- gamSim(1, n=n, scale=.15, dist="poisson")                ## simulate data
dat <- transform(dat, x4 = runif(n, 0, 1), x5 = runif(n, 0, 1)) ## spurious
b <- gam(y ~ s(x0) + s(x1) + s(x2) + s(x3) + s(x4) + s(x5), data = dat,
         family=poisson, select = TRUE, method = "REML")
#summary(b)
#plot(b, pages = 1)
```
```{r shrinkage-example-truth}
p1 <- ggplot(dat, aes(x = x0, y = f0)) + geom_line()
p2 <- ggplot(dat, aes(x = x1, y = f1)) + geom_line()
p3 <- ggplot(dat, aes(x = x2, y = f2)) + geom_line()
p4 <- ggplot(dat, aes(x = x3, y = f3)) + geom_line()
plot_grid(p1, p2, p3, p4, ncol = 2, align = "vh", labels = paste0("x", 1:4))
```

***

- Simulate Poisson counts
- 4 known functions (left)
- 2 spurious covariates (`runif()` & not shown)

Example
=======
```{r shrinkage-example-summary}
summary(b)
```

Example
=======
```{r shrinkage-example-plot, fig.width = 16, fig.height = 9}
plot(b, scheme = 1, pages = 1)
```

Confidence intervals for smooths
================================
type:section

Confidence intervals for smooths
================================

`plot.gam()` produces approximate 95% intervals (at +/- 2 SEs)

What do these intervals represent?

Nychka (1988) showed that standard Wahba/Silverman type Bayesian confidence intervals on smooths had good **across-the-function** frequentist coverage properties.

Confidence intervals for smooths
================================

Marra & Wood (2012) extended this theory to the generalised case and explain where the coverage properties failed:

*Musn't over-smooth too much, which happens when $\lambda_j$ are over-estimated*

Two situations where this might occur

1. where true effect is almost in the penalty null space, $\hat{\lambda}_j \rightarrow \infty$
2. where $\hat{\lambda}_j$ difficult to estimate due to highly correlated covariates
    - if 2 correlated covariates have different amounts of wiggliness, estimated effects can have degree of smoothness *reversed*

Don't over-smooth
=================

> <small>In summary, we have shown that Bayesian componentwise variable width intervals... for the smooth components of an additive model **should achieve close to nominal *across-the-function* coverage probability**, provided only that we do not over-smooth so heavily... Beyond this  requirement not to oversmooth too heavily, the results appear to have rather weak dependence on smoothing parameter values, suggesting that the neglect of smoothing parameter  variability  should  not  significantly  degrade  interval  performance.</small>

Confidence intervals for smooths
================================

Marra & Wood (2012) suggested a solution to situation 1., namely true functions close to the penalty null space.

Smooths are normally subject to *identifiability* constraints (centred), which leads to zero variance where the estimated function crosses the zero line.

Instead, compute intervals for $j$ th smooth as if it alone had the intercept; identifiability constraints go on the other smooth terms.

Use `seWithMean = TRUE` in call to `plot.gam()`

Example
=======

```{r setup-confint-example, fig = TRUE, fig.width = 11, fig.height = 10, results = "hide"}
library(mgcv)
set.seed(0)
## fake some data...
f1 <- function(x) {exp(2 * x)}
f2 <- function(x) { 
  0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10 
}
f3 <- function(x) {x*0}

n<-200
sig2 <- 12
x0 <- rep(1:4,50)
x1 <- runif(n, 0, 1)
x2 <- runif(n, 0, 1)
x3 <- runif(n, 0, 1)
e <- rnorm(n, 0, sqrt(sig2))
y <- 2*x0 + f1(x1) + f2(x2) + f3(x3) + e
x0 <- factor(x0)

## fit and plot...
b <- gam(y ~ x0 + s(x1) + s(x2) + s(x3))

op <- par(mar = c(4,4,1,1) + 0.1)
layout(matrix(1:9, ncol = 3, byrow = TRUE))
curve(f1)
curve(f2)
curve(f3)
plot(b, shade=TRUE)
plot(b, shade = TRUE, seWithMean = TRUE) ## better coverage intervals
layout(1)
par(op)
```

p values for smooths
====================
type:section

p values for smooths
====================

...are approximate:

1. they don't account for the estimation of $\lambda_j$ --- treated as known, hence *p* values are biased low
2. rely on asymptotic behaviour --- they tend towards being right as sample size tends to $\infty$

Also, *p* values in `summary.gam()` have changed a lot over time --- all options except current default are deprecated as of `v1.18-13`.

The approach described in Wood (2006) is "*no longer recommended*"!

p values for smooths
====================

...are a test of **zero-effect** of a smooth term

Default *p* values rely on theory of Nychka (1988) and Marra & Wood (2012) for confidence interval coverage.

If the Bayesian CI have good across-the-function properties, Wood (2013a) showed that the *p* values have

- almost the correct null distribution
- reasonable power

Test statistic is a form of $\chi^2$ statistic, but with complicated degrees of freedom.

p values for unpenalized smooths
================================

The results of Nychka (1988) and Marra & Wood (2012) break down if smooth terms are unpenalized.

This include i.i.d. Gaussian random effects, (e.g. `bs = "re"`.)

Wood (2013b) proposed instead a test based on a likelihood ratio statistic:

- the reference distribution used is appropriate for testing a $\mathrm{H}_0$ on the boundary of the allowed parameter space...
- ...in other words, it corrects for a $\mathrm{H}_0$ that a variance term is zero.

p values for smooths
====================

have the best behaviour when smoothness selection is done using **ML**, then **REML**.

Neither of these are the default, so remember to use `method = "ML"` or `method = "REML"` as appropriate

p values for parametric terms
=============================

...are based on Wald statistics using the Bayesian covariance matrix for the coefficients.

This is the "right thing to do" when there are random effects terms present and doesn't really affect performance if there aren't.

Hence in most instances you won't need to change the default `freq = FALSE` in `summary.gam()`

anova()
===========
type:section

anova()
===========

**mgcv** provides an `anova()` method for `"gam"` objects:

1. Single model form: `anova(m1)`
2. Multi model form: `anova(m1, m2, m3)`

anova() --- single model form
=============================

This differs from `anova()` methods for `"lm"` or `"glm"` objects:

- the tests are Wald-like tests as described for `summary.gam()` of a $\mathrm{H}_0$ of zero-effect of a smooth term
- these are not *sequential* tests!

anova()
===========

```{r anova-example-single, echo = TRUE}
b1 <- gam(y ~ x0 + s(x1) + s(x2) + s(x3), method = "REML")
anova(b1)
```

anova() --- multi model form
=============================

The multi-model form should really be used with care --- the *p* values are really *approximate*

```{r anova-example-multi, echo = TRUE}
b1 <- gam(y ~ s(x0) + s(x1) + s(x2) + s(x3) + s(x4) + s(x5),
          data = dat, family=poisson, method = "ML")
b2 <- update(b1, . ~ . - s(x3) - s(x4) - s(x5))
anova(b2, b1, test = "Chisq")
```

anova() --- multi model form
=============================

The multi-model form should really be used with care --- the *p* values are really *approximate*

*Don't used for testing random effect splines!*

For *general smooths* deviance is replaced by $-2\mathcal{L}(\hat{\beta})$

AIC for GAMs
============
type:section

AIC for GAMs
============

- Comparison of GAMs by a form of AIC is an alternative frequentist approach to model selection
- Rather than using the marginal likelihood, the likelihood of the $\mathbf{\beta}_j$ *conditional* upon $\lambda_j$ is used, with the EDF replacing $k$, the number of model parameters
- This *conditional* AIC tends to select complex models, especially those with random effects, as the EDF ignores that $\lambda_j$ are estimated
- Wood et al (2016) suggests a correction that accounts for uncertainty in $\lambda_j$

$$
AIC = -2l(\hat{\beta}) + 2\mathrm{tr}(\widehat{\mathcal{I}}V^{'}_{\beta})
$$

AIC
====================

In this example, $x_3$, $x_4$, and $x_5$ have no effects on $y$

```{r aic-example, echo = TRUE}
AIC(b1, b2)
```

When there is *no difference* in compared models, accepts larger model ~16% of the time: consistent with probability AIC chooses a model with 1 extra spurious parameter $Pr(\chi^2_1 > 2)$

```{r aic-chisq, echo = TRUE}
pchisq(2, 1, lower.tail = FALSE)
```

References
==========
- [Marra & Wood (2011) *Computational Statistics and Data Analysis* **55** 2372--2387.](http://doi.org/10.1016/j.csda.2011.02.004)
- [Marra & Wood (2012) *Scandinavian Journal of Statistics, Theory and Applications* **39**(1), 53--74.](http://doi.org/10.1111/j.1467-9469.2011.00760.x.)
- [Nychka (1988) *Journal of the American Statistical Association* **83**(404) 1134--1143.](http://doi.org/10.1080/01621459.1988.10478711)
- Wood (2006, 2018) *Generalized Additive Models: An Introduction with R*. Chapman and Hall/CRC. (New 2nd Edition)
- [Wood (2013a) *Biometrika* **100**(1) 221--228.](http://doi.org/10.1093/biomet/ass048)
- [Wood (2013b) *Biometrika* **100**(4) 1005--1010.](http://doi.org/10.1093/biomet/ast038)
- [Wood et al (2016) *JASA* **111** 1548--1563](https://doi.org/10.1080/01621459.2016.1180986)
